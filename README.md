# `klay_beam` â€“Â Engineering README


## ğŸ“œÂ Preface

This README is the **canonical, auditâ€‘ready** introduction to the KlayÂ Beam platform.â€¯It is intentionally verbose so that:

* **New engineers** ramp up without spelunking in Slack threads.
* **Auditors** can validate dataâ€‘flow guarantees and dependency hygiene.
* **Ops** can debug production pipelines at 03:00 without paging devs.

> If you add a feature or change a process, **update this file in the same PR** so we never drift. Compliance depends on it.


## ï¸ğŸ›ï¸Â Overview

KlayÂ Beam is our inâ€‘house framework for *massively* parallel audio processing.Â It marries three layers:

| Layer            | Responsibility                                                                                                                            |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **ApacheÂ Beam**  | Declarative DAG that scales from a laptop (`DirectRunner`) to thousands of machines (`DataflowRunner`).                                   |
| **Docker**       | Hermetic runtimes for local tests *and* remote workers; the identical imageâ€‘SHA runs everywhere, eliminating â€œworksâ€‘onâ€‘myâ€‘machineâ€ drift. |
| **GCPÂ Dataflow** | Spotâ€‘provisioned Compute Engine VMs that pull the Beam container, hydrate it, and execute the pipeline shards in parallel.                |

> **Why Beam?** One SDK targets local threads, an onâ€‘prem Mesos cluster, *or* fullyâ€‘managed Dataflowâ€”no rewrite required.

### ï¸ğŸÂ Data lifecycle (10â€¯000â€‘ft)

1. **Ingest** â€“ `MatchFiles` emits `FileMetadata` for every object that matches a glob.
2. **Transform** â€“ loaders decode audio â†’ tensors; `PTransform`s perform DSP, ML inference, feature extraction.
3. **Persist** â€“ results land in CloudÂ Storage / BigQuery / Firestore depending on the pipeline.
4. **Observe** â€“ CloudÂ Logging, Profiler, ErrorÂ Reporting capture metrics & failures.


## ğŸ§©Â Repository Anatomy

```
.
â”œâ”€â”€ README.md          â†Â This document
â”œâ”€â”€ klay_beam/         â†Â Core Python package *+* base Docker image
â””â”€â”€ jobs/              â†Â One subâ€‘dir per job
    â”œâ”€â”€ job_whisper/   â†Â GPUâ€‘heavy Whisper speechâ€‘toâ€‘text
    â”œâ”€â”€ job_byt5/      â†Â CPUâ€‘only BYT5 tagging
    â””â”€â”€ â€¦
```

*Every* path in `jobs/**` is an **independent** Python package.Â A job *may* add a `Dockerfile` that layers on top of the **base image** shipped in `klay_beam`â€”but only when truly required (see below).


## ğŸ—ï¸Â Jobs

### What *is* a job?

A **job** is a selfâ€‘contained Beam pipeline with a single business goal (e.g. stem separation, lyrics extraction, embedding generation).Â Each lives under `jobs/job_<name>/` and must provide:

| File/Dir                                | Purpose                                                                                                  |
| --------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| `Dockerfile`                            | Adds *only* the deltas (models, extra wheels, GPU drivers) on top of the base image.                     |
| `Makefile`                              | Canonical interface for humans *and* CI. Targets:                                                        |
| â€¢Â `code-style`Â Â·Â `type-check`Â Â·Â `tests` |                                                                                                          |
| â€¢Â `docker`Â Â·Â `docker-push`              |                                                                                                          |
| â€¢Â `run-local`Â Â·Â `run-dataflow`          |                                                                                                          |
| `bin/`                                  | Thin CLI wrappers (usually `run_workflow.py`) that assemble `PipelineOptions` and launch the Beam graph. |
| `environment/`                          | *Optional* **condaâ€‘lock** for heavyweight native deps that cannot be `pip`â€‘installed.                    |
| `src/`                                  | Package code (`import job_whisper â€¦`).                                                                   |
| `tests/`                                | PyTest suite; required for merge (â‰¥â€¯80â€¯% new branch coverage).                                           |
| `README.md`                             | Short operational doc (mirrors Makefile flags).                                                          |

> **Philosophy** â€“ Extend the base image **only when** you need a private model, conflicting dependency versions, or GPUâ€‘specific binaries.Â Otherwise inherit directly; containers stay smallÂ ğŸƒâ€â™€ï¸.

### CPU vs GPU templates

| Scenario                 | Template                                                                                                     |
| ------------------------ | ------------------------------------------------------------------------------------------------------------ |
| CPUâ€‘only workload        | **`jobs/job_byt5`** â€“ lean, singleâ€‘thread performance paramount.                                             |
| GPUâ€‘accelerated workload | **`jobs/job_whisper`** â€“ shows how to request NVIDIAÂ L4s and install drivers via `dataflow_service_options`. |

### Creating a new jobÂ ğŸš€

1. **Scaffold**

   ```bash
   cp -r jobs/job_byt5 jobs/job_mynew
   find jobs/job_mynew -type f -exec sed -i '' -e 's/job_byt5/job_mynew/g' {} +
   ```
2. **Update metadata** â€“Â `pyproject.toml` name is already patched; set version â†’ `0.1.0` in `__init__.py`.
3. **Decide on Dockerfile**

   | Question                                    | If **yes** ..                                  |
   | ------------------------------------------- | ---------------------------------------------- |
   | Need extra *system* libs?                   | Write a Dockerfile and `apt-get` them.         |
   | Need extra *Python* deps? (pipâ€‘installable) | Add to `pyproject.toml`; no Dockerfile needed. |
   | Need private local packages or big models?  | Write a Dockerfile and `COPY` / install.       |

   Always start `FROM ${BASE_IMAGE}` (echo it with `make print-base`).
4. **Pipeline code** â€“ in `bin/run_workflow.py`.Â Leverage existing transforms in **`klay_beam.transforms`** before writing new ones.Â Custom transforms â†’ `src/job_mynew/transforms.py`.
5. **Tests** â€“Â `pytest -q`; aim â‰¥â€¯80â€¯% coverage for new code.
6. **Commit & tag**

   ```bash
   git add .
   git commit -m 'job-mynew: 0.1.0'
   git tag -a job-mynew-0.1.0 -m 'Initial release'
   git push --tags
   ```

   Tagging triggers **CloudÂ Build** â†’ pushes `us-docker.pkg.dev/â€¦/job-mynew:0.1.0`.

### Launching jobs

| Environment  | Command                                                           |
| ------------ | ----------------------------------------------------------------- |
| **Local**    | `make run-local match_pattern="~/audio/**.wav" audio_suffix=.wav` |
| **Dataflow** | `make run-dataflow job_name=mynew max_num_workers=128 \`          |

```
          Â Â `match_pattern='gs://bucket/audio/**.wav' audio_suffix=.wav` |
```

All runtime knobs (`--max_num_workers`, `--runner`, `--temp_location`, â€¦) live in the Makefile; override them inline via `make VAR=value â€¦`.


# ğŸ“¦Â `klay_beam` Core Library

* **Reusable transforms** â€“ fileÂ IO, resampling, channel ops, `SkipCompleted`, fileâ€‘name mutation, `FeatureWriter`, etc.
  Undocumented gems worth noting:

  * **`SkipCompleted`** â€“ checks output path and drops elements already processed; idempotent reruns cost \$0.
  * **`FileNameMutator`** â€“ atomic `gs://bucket/{fname}` renames that survive preâ€‘emption.
  * **`FeatureWriter`** â€“ writes NumPy/Parquet features with schema injection.
* **Utility CLI** â€“ `python -m klay_beam.run_example` showcases an endâ€‘toâ€‘end local pipeline (decode â†’ duration sum).
* **Scripts** â€“ `bin/create_test_audio_files.py` generates synthetic WAV/OGG for smoke tests; `bin/test-soundfile.py` validates libsndfile linkages.
* **Base Docker image** â€“ `klay_beam/Dockerfile.conda` is our golden runtime â€“ every job inherits from this.
* **Dev helpers** â€“ `docker-env-helper.sh` computes build args (`PY_VERSION`, `TORCH_VERSION`, etc.) for CI matrix.

## Local development

| Task                    | Command                                                                                          |
| ----------------------- | ------------------------------------------------------------------------------------------------ |
| **Createâ€¯env**          | `git submodule update --init --recursive && conda env create -f environment/py3.10-torch2.5.yml` |
| **RunÂ tests**           | `make tests`                                                                                     |
| **Lintâ€¯+â€¯format**       | `make code-style` *(flake8 + blackÂ --check)*                                                     |
| **Staticâ€¯types**        | `make type-check` *(mypy)*                                                                       |
| **Generate condaâ€‘lock** | `make conda-lock` whenever `environment/*.yml` changes                                           |

## Release process

1. Implement feature/bugfix â†’ ensure tests pass.
2. Bump `src/klay_beam/__init__.py::__version__`, update `CHANGELOG.md`.
3. `git tag v$(./get_version.sh)` & push.
4. GitHub Actions publishes to **PyPI** (`publish-pypi` branch) then builds **DockerHub** images (`publish-docker-hub`).

Every published image tag encodes:
`klay_beam-${VERSION}-py${PY_VERSION}-beam${BEAM_VERSION}-torch${TORCH_VERSION}[-cuda${CUDA_VERSION}]`


# ğŸ”§Â DevOps Pipeline

## Base image (GitHubÂ Actions)

* **Workflow** â€“ `.github/workflows/publish-docker-hub.yaml`.
* **Generate lock** â€“ `make conda-lock` when env file changes.Â CI fails if lock is outâ€‘ofâ€‘date.
* **Matrix** â€“ PythonÂ 3.10 Ã— TorchÂ {2.1,â€¯2.5} Ã— {CPU,â€¯CUDAâ€¯12.1} Ã— Beamâ€¯`${BEAM_VERSION}`.
* **Secrets** â€“ SSH deploy keys for private submodules, DockerHub creds.
* **Multiâ€‘stage build** â€“ `Dockerfile.conda`:

  1. Pull Beam SDK image (for worker harness binaries).
  2. Build conda env with **mambaforge**.
  3. Produce slim runtime (`python:3.10-slim-bookworm`) + `RUN_PYTHON_SDK_IN_DEFAULT_ENVIRONMENT=1`.

## Job images (CloudÂ Build)

* **Trigger** â€“ `git tag job-<name>-<semver>` â†’ `git push --tags`.
* **Steps** â€“ `cloudbuild.yaml` clones submodules, builds `jobs/job_<name>/Dockerfile`, pushes to Artifact Registry (`us-docker.pkg.dev/klay-home/klay-docker`).
* **Image TTL** â€“ immutable; older tags never deleted (audit trail).

## Versioning rules

| Artifact            | How to bump                                                              |
| ------------------- | ------------------------------------------------------------------------ |
| **klay\_beam**      | semantic version in `src/klay_beam/__init__.py` â†’ tag `vX.Y.Z`.          |
| **Job**             | bump `src/job_<name>/__init__.py::__version__` â†’ tag `job-<name>-X.Y.Z`. |
| **DockerÂ envÂ lock** | regenerate `conda-lock` â†’ commit (hash appears in image tag).            |

## Continuous quality gates

* Push â†’ PR runs `pytest`, `flake8`, `blackÂ --check`, `mypy`.
* `main` branch is protected; must stay **green**.


# ğŸ§ Â Troubleshooting

1. **Reproduce locally** with the exact container:

   ```bash
   ```

docker run -it --entrypoint /bin/bash gcr.io/<region>/klay-beam:<tag>

```
2. **Inspect logs** â€“ CloudÂ Logging, filter `labels.dataflow.googleapis.com/step_id`.

## Common issues & fixes
| Symptom | Likely cause | Resolution |
| ------- | ------------ | ---------- |
| Pipeline hangs at start | Fusion across huge `MatchFiles` | Insert `beam.Reshuffle()` after glob. |
| `No CUDA devices` | Wrong `machine_type` or `worker_accelerator` | Match GPU type (`nvidia-l4`, `a100`, etc.) + quota. |
| `Permission denied` when writing temp | SA lacks `storage.objectAdmin` on temp bucket | Grant role or change `--temp_location`. |
| `ErrImagePull` | Image tag typo or not pushed yet | Verify image exists in Artifact Registry and tag matches lock hash. |
| `ModuleNotFoundError` for private lib | Dockerfile forgets to COPY submodule | Add `COPY --from=builder /submodules/â€¦` or `pip install -e`. |
| Beam creates new venv | Missing `RUN_PYTHON_SDK_IN_DEFAULT_ENVIRONMENT=1` | Ensure env var in image. |

---

> Remember: **every README is a compliance artifact**. Keep it current so auditors stay happy and *nobody dies*Â ğŸ¤
